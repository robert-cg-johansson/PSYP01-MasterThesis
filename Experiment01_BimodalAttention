###############################################
### Draft Analysis Plan PSYP:01 Experiment 1 ###
###############################################

# Set up workspace
setwd("C:/ExcercisesforR")
rm(list=ls())

#Packages
library(ggplot2)
library(retimes)
library(tidyverse)
library(BayesFactor)
library(sft)

####################
# Simulate Dataset #
####################

#Set trials/subject and units random draws from exgauss

n            <- 12
trials_per_n <- 180
trials       <- trials_per_n * n


#soft
S <- rexgauss(trials, mu = 430, sigma=90, tau=200, positive = T)
S <- S[S > 0]
S <-  tapply(S, rep(seq_along(S), each = trials_per_n, length.out = length(S)), mean)

#loud
L <- rexgauss(trials, mu = 360, sigma=90, tau=200, positive = T)
L <- L[L > 0]
L <- tapply(L, rep(seq_along(L), each = trials_per_n, length.out = length(L)), mean)

#dim
D <- rexgauss(trials, mu = 480, sigma=90, tau=200, positive = T)
D <- D[D > 0]
D <- tapply(D, rep(seq_along(D), each = trials_per_n, length.out = length(D)), mean)

#bright
B <- rexgauss(trials, mu = 420, sigma=90, tau=200, positive = T)
B <- B[B > 0]
B <- tapply(B, rep(seq_along(B), each = trials_per_n, length.out = length(B)), mean)

#soft + dim
SD <- rexgauss(trials, mu = 400, sigma=90, tau=200, positive = T)
SD <- SD[SD > 0]
SD <- tapply(SD, rep(seq_along(SD), each = trials_per_n, length.out = length(SD)), mean)

#soft + bright
SB <- rexgauss(trials, mu = 360, sigma=90, tau=200, positive = T)
SB <- SB[SB > 0]
SB <- tapply(SB, rep(seq_along(SB), each = trials_per_n, length.out = length(SB)), mean)

#dim + loud
DL <- rexgauss(trials, mu = 350, sigma=90, tau=200, positive = T)
DL <- DL[DL > 0]
DL <- tapply(DL, rep(seq_along(DL), each = trials_per_n, length.out = length(DL)), mean)

#loud + bright
LB <- rexgauss(trials, mu = 320, sigma=90, tau=200, positive = T)
LB <- LB[SD > 0]
LB <- tapply(LB, rep(seq_along(LB), each = trials_per_n, length.out = length(LB)), mean)


#Create dataframe
data = as.data.frame(c(S, L, D, B, SD, LB, DL, SB))
energy = c(rep("soft", n), rep("loud", n), rep("dim", n), rep("bright", n),
           rep("soft", n), rep("loud", n), rep("dim", n), rep("bright", n))
redundancy = c(rep("unimodal", n*4), rep("redundant", n*4))
rt.data = cbind(data, energy, redundancy)
names(rt.data) = c("reaction_time", "stimulus", "redundancy")

###########################################
# Survivor and Mean Interaction Contrasts #
###########################################

#Mean and Survivor Contrasts + Capacity Analysis as devised by 'SFT in R' (Houpt et al.2013)
#http://www.indiana.edu/~psymodel/papers/HouBla13.pdf

# Simulate single channel response times
T1.h <- B
T1.l <- D
T2.h <- L
T2.l <- S

# Combine into "observed" response times
# assuming a Serial-AND model
hh <- T1.h + T2.h
hl <- T1.h + T2.l
lh <- T1.l + T2.h
ll <- T1.l + T2.l
# Run the SIC analysis
SerialAND <- sic(hh,hl,lh,ll)

SerialAND$SIC

SerialAND$Dominance

SerialAND$positive

SerialAND$negative
SerialAND$MIC

plot(SerialAND$SIC, do.p=FALSE)

########################
# Capacity coefficient #
########################

#OR-coefficient

# Generate single source response times
rate1 <- 1/800
rate2 <- 1/600
pa <- rweibull(100, shape=2,
                 scale=1/rate1)
ap <- rweibull(100, shape=2,
                 scale=1/rate2)

# Simulate a limited capacity
# Parallel-OR model
# Limited capacity means slower
# processing when there are multiple
# sources, so we use .5 times the
# original rate.
pp.1 <- rweibull(100, shape=2,
                   scale=1/(.5*rate1))
pp.2 <- rweibull(100, shape=2,
                   scale=1/(.5*rate2))

pp <- pmin( pp.1, pp.2 )

# Run the capacity analysis
cap <- capacity.or(list(pp, pa, ap))
cap$Ctest

plot(cap$Ct, xlim=c(0,2000), ylim=c(0,2))

#AND-coefficient

#############################
### Race Model Inequality ###
#############################

# Race Model Inequality as devised by Ulrich et al. (2007) 
# https://link.springer.com/article/10.3758/BF03193160?LI=true

# RMI algorithm kindly translated from MatLab to R by Y. Lin & L. Piwek via StackExchange. 
# https://stats.stackexchange.com/questions/48581/testing-the-race-model-inequality-in-r

# cx=channel 1, cy=channel 2, cz=redundant 
cx <- S
cy <- D
cz <- SD

#calculate quantiles corresponding to the given probabilities
#I use the default quantile algorithm (type 7 see R manual for 'quantile')

psq <- seq(0,1,0.1)
gx <- quantile(cx, probs=psq, names=F)
gy <- quantile(cy, probs=psq, names=F)
gz <- quantile(cz, probs=psq, names=F)

#here I visualize the quantiles, looks more or less like in Urlich etal 2007
xyz <- data.frame(rt=c(gx,gy,gz), 
                  perc=rep(psq,3), 
                  cond=rep(c("gx(t)", "gy(t)","gz(t)"), each=length(psq)))
require(lattice)
xyplot(perc ~ rt, groups = cond, xyz, col=1, lty=3:1, type=c("g","l","p"), 
       key=list(lines=list(lty=3:1), text=list(labels=levels(xyz$cond))))


#Compute the bounding um 'b' from g(x) and g(y)
b = gx + gy
#but then when I visualize it, it's wrong:

xyz <- data.frame(rt=c(gx,gy,gz,b), 
                  perc=rep(psq,4), 
                  cond=rep(c("gx(t)", "gy(t)","gz(t)","gx(t)+gy(t)"), each=length(psq)))

xyplot(perc ~ rt, groups = cond, xyz, col=1, lty=4:1, type=c("g","l","p"), 
       key=list(lines=list(lty=4:1), text=list(labels=levels(xyz$cond))))


#############Alternative solution

#Probspace function
probSpace <- function(len){ 
  # PROBSPACE Determine the interval of percentiles 
  #   The function used equation (3) in Ulrich, Miller and Schroter (2007)
  P <- numeric(len);
  for(i in 1:len){
    P[i] <- (i - .5) / len;
  }
  return(P)
}


##CDF function
cdf.ulrich <- function(data=NULL, maximum=3000){
  # Create a container, whose length is the longest data vector
  # data is the output from `ties` function
  U <- data[,1]
  R <- data[,2]
  C <- data[,3]
  G <- numeric(maximum);
  
  
  # The length of the processed data vector, trimming off ties, if there is any.
  k <- length(U);  # U contains data in millisecond, e.g., 320 ms etc. 
  
  # The last element of the cumulative frequency supposely is the 
  # length of the data vector.
  n <- C[k]
  
  for(i in 1:k) { U[i] <- round(U[i]) }
  
  # from 1 ms to the first value of the data set, their probability should be 0.
  for(t in 1:U[1]) { G[t] <- 0 }   
  
  for(t in U[1]:U[2]){
    G[t] <- ( R[1]/2 + (R[1]+R[2]) / 2*(t-U[1]) / (U[2] - U[1]) ) / n;
  }
  
  for(i in 2:(k-1)){
    for(t in U[i]:U[i+1]){
      G[t] <- (C[i-1] + R[i] / 2+(R[i] +R[i+1]) / 2*(t-U[i]) / (U[i+1] - U[i])) / n;
    }
  }
  
  for(t in U[k]:maximum){
    G[t] <- 1;
  }
  return(G)
}

##ties function
ties <- function(W){
  # Count number k of unique values
  # and store these values in U.
  U <- NULL; W <- sort(W); n = length(W); k = 1; U[1] <- W[1]
  for (i in 2:n) {
    if (W[i] != W[i-1]) {
      k <- k+1;
      U <- cbind(U, W[i])
    }
  }
  U <- U[1,]
  
  # Determine number of replications R
  # k is the length of the vector, after trimming off the ties
  R <- numeric(k) 
  for (i in 1:k){
    for (j in 1:n){
      if (U[i] == W[j]) R[i] <- R[i] + 1;
    }
  }
  
  # Determine the cumlative frequency
  C <- numeric(k) 
  C[1] <- R[1]
  for(i in 2:k){
    C[i] <- C[i-1] + R[i];
  }
  res <- list(U, R, C)
  names(res) <- c("U", "R", "C")
  return(as.data.frame(res))
}
##Get percentile function
GetPercentile <- function( P, G, tmax ){
  # Determine minimum of |G(Tp[i]) - P[i]|
  np <- length(P);
  Tp <- numeric(np)
  for( i in 1:np) {
    cc <- 100;
    for(t in 1:tmax) {
      if ( abs(G[t] - P[i]) < cc ) {
        c <- t;
        cc <- abs(G[t] - P[i]);
      }
    }
    
    if( P[i] > G[c] ){
      Tp[i] <-  c + (P[i] - G[c]) / (G[c+1] - G[c]);
    } else {
      Tp[i] <- c + (P[i] - G[c]) / (G[c] - G[c-1]);
    }
  }
  return( Tp )
}


##RMi
psq <- probSpace(10); psq

dfx <- ties(cx)
dfy <- ties(cy)
dfz <- ties(cz)
tmax <- max(cx,cy,cz)

gx <- cdf.ulrich(data=dfx, maximum=tmax)
gy <- cdf.ulrich(data=dfy, maximum=tmax)
gz <- cdf.ulrich(data=dfz, maximum=tmax)

b <- gx + gy
xp <- GetPercentile(psq, gx, tmax)
yp <- GetPercentile(psq, gy, tmax)
zp <- GetPercentile(psq, gz, tmax);
bp <- GetPercentile(psq, b, tmax);

gdf <- data.frame(RT =c(xp,yp,zp,bp), Probability =rep(psq, 4),
                  Condition =rep(c("gx(t)", "gy(t)","gz(t)","gx(t)+gy(t)"), each=length(xp)))
panelf <- ggplot(gdf, aes(x = RT, y = Probability, group=Condition,
                          colour=Condition, shape=Condition)) + 
  geom_point() + geom_line() 
panelf + coord_cartesian(xlim = c(500, 750), ylim=c(-.01,1.01)) +
  theme(legend.position= c(.85, .20),  
        legend.title = element_text(size=12),
        legend.text = element_text(size=12))
zp <- GetPercentile(psq, gz, tmax);
bp <- GetPercentile(psq, b, tmax);

