################################################
### Draft Analysis Code PSYP:01 Experiment 1 ###
################################################

#Set up workspace
setwd("C:/R.working.directory/")
rm(list=ls())

#Packages
library(ggplot2)
library(BayesFactor)
library(sft)
library(rstan)
library(bridgesampling)

####################
# Simulate Dataset #
####################

#Create simulation function for correlated RT and RF data

response_simulation = function(
  trials,
  mean_RT,
  mean_RF,
  sd_RT,
  sd_RF,
  r
){
  responses = mvrnorm(n = trials,
                      mu = c(0, 0),
                      Sigma = matrix(c(1, r,
                                       r, 1), nrow = 2))
  
  RT = responses[,1]*sd_RT+mean_RT
  RF = responses[,2]*sd_RF+mean_RF
  
  
  data = cbind(RT, RF)
  
  return(data)
}


#set number of trials per condition
tpc <- 40 

#Simulate data

#Soft
S <-  response_simulation(trials = tpc,
                          mean_RT = 430,
                          mean_RF = 15,
                          sd_RT = 90,
                          sd_RF = 2,
                          r = -0.3)

#Loud
L <-  response_simulation(trials = tpc,
                          mean_RT = 360,
                          mean_RF = 16,
                          sd_RT = 90,
                          sd_RF = 2,
                          r = -0.3)
#Dim
D <-  response_simulation(trials = tpc,
                          mean_RT = 480,
                          mean_RF = 15,
                          sd_RT = 90,
                          sd_RF = 2,
                          r = -0.3)
#Bright
B <-  response_simulation(trials = tpc,
                          mean_RT = 420,
                          mean_RF = 15,
                          sd_RT = 90,
                          sd_RF = 2,
                          r = -0.3)
#Soft + Dim
SD <-  response_simulation(trials = tpc,
                           mean_RT = 400,
                           mean_RF = 15,
                           sd_RT = 90,
                           sd_RF = 2,
                           r = -0.3)
#Soft + Bright
SB <-  response_simulation(trials = tpc,
                           mean_RT = 360,
                           mean_RF = 15,
                           sd_RT = 90,
                           sd_RF = 2,
                           r = -0.3)
#Loud + Dim
LD <-  response_simulation(trials = tpc,
                           mean_RT = 350,
                           mean_RF = 15,
                           sd_RT = 90,
                           sd_RF = 2,
                           r = -0.3)
#Loud + Bright
LB <-  response_simulation(trials = tpc,
                           mean_RT = 320,
                           mean_RF = 15,
                           sd_RT = 90,
                           sd_RF = 2,
                           r = -0.3)


########################
# Analysis of Variance #
########################

#Create a separate RT dataset for ANOVA
data = as.data.frame(c(S[,1], L[,1], D[,1], B[,1], SD[,1], LD[,1], SB[,1], LB[,1]))
energy = c(rep("soft", tpc), rep("loud", tpc), rep("dim", tpc), rep("bright", tpc),
           rep("soft_dim", tpc), rep("loud_dim", tpc), rep("soft_bright", tpc), rep("loud_bright", tpc))
redundancy = c(rep("unimodal", tpc*4), rep("redundant", tpc*4))
RT.data = cbind(data, energy, redundancy)
names(RT.data) = c("reaction_time", "stimulus", "redundancy")

#Compute Bayes Factors for RT data
bayes_factor <- anovaBF(reaction_time ~ stimulus*redundancy, data=RT.data)

print(bayes_factor)
bayes_factor@bayesFactor$bf

#Create a separate RF dataset for ANOVA
data = as.data.frame(c(S[,2], L[,2], D[,2], B[,2], SD[,2], LD[,2], SB[,2], LB[,2]))
energy = c(rep("soft", tpc), rep("loud", tpc), rep("dim", tpc), rep("bright", tpc),
           rep("soft_dim", tpc), rep("loud_dim", tpc), rep("soft_bright", tpc), rep("loud_bright", tpc))
redundancy = c(rep("unimodal", tpc*4), rep("redundant", tpc*4))
RF.data = cbind(data, energy, redundancy)
names(RF.data) = c("response_force", "stimulus", "redundancy")

#Compute Bayes Factors for RF data
bayes_factor <- anovaBF(response_force ~ stimulus*redundancy, data=RF.data)

print(bayes_factor)
bayes_factor@bayesFactor$bf

#################################
# Inverse Effectiveness Theorem #
#################################

set.seed(12345)

#Create difference scores for the redundancy gain contrast
auditory_gain_contrast <- ((S[,1]-SD[,1]) - (L[,1]-LB[,1]))  
visual_gain_contrast   <- ((B[,1]-LB[,1]) - (D[,1]-SD[,1]))   

#The principle of inverse effectiveness of multisensory integration predicts these contrast to be positive

#Standardize data
y <- auditory_gain_contrast
y <- y/sd(y)
n <- length(y)

#Specify models
stancodeH0 <- '
data {
  int<lower=1> n; // number of observations
  vector[n] y; // observations
}
parameters {
  real<lower=0> sigma2; // variance parameter
}
model {
  target += log(1/sigma2); // Jeffreys prior on sigma2
  target += normal_lpdf(y | 0, sqrt(sigma2)); // likelihood
}
'
stancodeH1 <- '
data {
  int<lower=1> n; // number of observations
  vector[n] y; // observations
  real<lower=0> r; // Cauchy prior scale
}
parameters {
  real delta;
  real<lower=0> sigma2;// variance parameter
}
model {
  target += cauchy_lpdf(delta | 0, r); // Cauchy prior on delta
  target += log(1/sigma2); // Jeffreys prior on sigma2
  target += normal_lpdf(y | delta*sqrt(sigma2), sqrt(sigma2));  // likelihood
}
'

#Compile models
stanmodelH0 <- stan_model(model_code = stancodeH0, model_name="stanmodel")
stanmodelH1 <- stan_model(model_code = stancodeH1, model_name="stanmodel")

#Fit models
stanfitH0 <- sampling(stanmodelH0, data = list(y = y, n = n),
                      iter = 10000, warmup = 1000, chains = 3, cores = 1,
                      control = list(adapt_delta = .99))
stanfitH1 <- sampling(stanmodelH1, data = list(y = y, n = n, r = 1/sqrt(2)),
                      iter = 20000, warmup = 1000, chains = 4, cores = 1,
                      control = list(adapt_delta = .99))


#Compute log marginal likelihoods
H0 <- bridge_sampler(stanfitH0, silent = TRUE)
H1 <- bridge_sampler(stanfitH1, silent = TRUE)

print(H0)
print(H1)

H0.error <- error_measures(H0)$percentage
H1.error <- error_measures(H1)$percentage

print(H0.error)
print(H1.error)

BF10 <- bf(H1, H0)
print(BF10)


###########################################
# Survivor and Mean Interaction Contrasts #
###########################################

#Mean and Survivor Function Contrasts + Capacity Analysis as devised by Houpt et al. (2013)
#http://www.indiana.edu/~psymodel/papers/HouBla13.pdf

#Targets 1+2 (high/low saliency) for UCIP testing
T1.h <- B[,1]
T1.l <- D[,1]
T2.h <- L[,1]
T2.l <- S[,1]

#Optional specification of UCIP (Unlimited Capacity Independent Processing) model
hh <- T1.h + T2.h
hl <- T1.h + T2.l
lh <- T1.l + T2.h
ll <- T1.l + T2.l

#Run the SIC analysis
SerialAND <- sic(hh,hl,lh,ll)
SerialAND$SIC
SerialAND$Dominance
SerialAND$positive
SerialAND$negative
SerialAND$MIC
plot(SerialAND$SIC, do.p=FALSE)

#Enter actual data
hh <- LB[,1]
hl <- LD[,1]
lh <- SB[,1]
ll <- SD[,1]

#Run the SIC analysis
SerialAND <- sic(hh,hl,lh,ll)
SerialAND$SIC
SerialAND$Dominance
SerialAND$positive
SerialAND$negative
SerialAND$MIC
plot(SerialAND$SIC, do.p=FALSE)

########################
# Capacity coefficient #
########################

#OR-coefficient (performance on trials with multiple targets as compared to UCIP(OR) prediction
#Input single source response times
pa <- L[,1]
ap <- B[,1]

#Input multiple source response time
pp <- LB[,1]

#Run the capacity analysis
cap <- capacity.or(list(pp, pa, ap))
cap$Ctest
plot(cap$Ct, xlim=c(0,1000), ylim=c(0,2), ylab = "C(t)", xlab="t", main="Capacity Coefficient")

#AND-coefficient (performance on trials with multiple targets as compared to UCIP(AND) prediction)

#Input single source response times
pa <- L[,1]
ap <- B[,1]

#Input multiple source response time
pp <- LB[,1]

cap <- capacity.and(list(pp, pa, ap), ratio=FALSE)

#Run the capacity analysis
cap <- capacity.or(list(pp, pa, ap))
cap$Ctest
plot(cap$Ct, xlim=c(0,1000), ylim=c(0,2), ylab = "C(t)", xlab="t", main="Capacity Coefficient")

#############################
### Race Model Inequality ###
#############################

#Race Model Inequality for RT distributions as devised by Ulrich et al. (2007) 
#https://link.springer.com/article/10.3758/BF03193160?LI=true

#RMI algorithm kindly translated from MatLab to R by Y. Lin & L. Piwek via StackExchange. 
#https://stats.stackexchange.com/questions/48581/testing-the-race-model-inequality-in-r

# cx=channel 1, cy=channel 2, cz=redundant trials
cx <- S[,1]
cy <- D[,1]
cz <- SD[,1]

#calculate quantiles corresponding to the given probabilities
#I use the default quantile algorithm (type 7 see R manual for 'quantile')

psq <- seq(0,1,0.1)
gx <- quantile(cx, probs=psq, names=F)
gy <- quantile(cy, probs=psq, names=F)
gz <- quantile(cz, probs=psq, names=F)

#here I visualize the quantiles, looks more or less like in Ulrich et al 2007
xyz <- data.frame(rt=c(gx,gy,gz), 
                  perc=rep(psq,3), 
                  cond=rep(c("gx(t)", "gy(t)","gz(t)"), each=length(psq)))
require(lattice)
xyplot(perc ~ rt, groups = cond, xyz, col=1, lty=3:1, type=c("g","l","p"), 
       key=list(lines=list(lty=3:1), text=list(labels=levels(xyz$cond))))


#Compute the bounding um 'b' from g(x) and g(y)
b = gx + gy
#but then when I visualize it, it's wrong:

xyz <- data.frame(rt=c(gx,gy,gz,b), 
                  perc=rep(psq,4), 
                  cond=rep(c("gx(t)", "gy(t)","gz(t)","gx(t)+gy(t)"), each=length(psq)))

xyplot(perc ~ rt, groups = cond, xyz, col=1, lty=4:1, type=c("g","l","p"), 
       key=list(lines=list(lty=4:1), text=list(labels=levels(xyz$cond))))


#############Alternative solution

#Probspace function
probSpace <- function(len){ 
  # PROBSPACE Determine the interval of percentiles 
  #   The function used equation (3) in Ulrich, Miller and Schroter (2007)
  P <- numeric(len);
  for(i in 1:len){
    P[i] <- (i - .5) / len;
  }
  return(P)
}


##CDF function
cdf.ulrich <- function(data=NULL, maximum=3000){
  # Create a container, whose length is the longest data vector
  # data is the output from `ties` function
  U <- data[,1]
  R <- data[,2]
  C <- data[,3]
  G <- numeric(maximum);
  
  
  # The length of the processed data vector, trimming off ties, if there is any.
  k <- length(U);  # U contains data in millisecond, e.g., 320 ms etc. 
  
  # The last element of the cumulative frequency supposely is the 
  # length of the data vector.
  n <- C[k]
  
  for(i in 1:k) { U[i] <- round(U[i]) }
  
  # from 1 ms to the first value of the data set, their probability should be 0.
  for(t in 1:U[1]) { G[t] <- 0 }   
  
  for(t in U[1]:U[2]){
    G[t] <- ( R[1]/2 + (R[1]+R[2]) / 2*(t-U[1]) / (U[2] - U[1]) ) / n;
  }
  
  for(i in 2:(k-1)){
    for(t in U[i]:U[i+1]){
      G[t] <- (C[i-1] + R[i] / 2+(R[i] +R[i+1]) / 2*(t-U[i]) / (U[i+1] - U[i])) / n;
    }
  }
  
  for(t in U[k]:maximum){
    G[t] <- 1;
  }
  return(G)
}

##ties function
ties <- function(W){
  # Count number k of unique values
  # and store these values in U.
  U <- NULL; W <- sort(W); n = length(W); k = 1; U[1] <- W[1]
  for (i in 2:n) {
    if (W[i] != W[i-1]) {
      k <- k+1;
      U <- cbind(U, W[i])
    }
  }
  U <- U[1,]
  
  # Determine number of replications R
  # k is the length of the vector, after trimming off the ties
  R <- numeric(k) 
  for (i in 1:k){
    for (j in 1:n){
      if (U[i] == W[j]) R[i] <- R[i] + 1;
    }
  }
  
  # Determine the cumlative frequency
  C <- numeric(k) 
  C[1] <- R[1]
  for(i in 2:k){
    C[i] <- C[i-1] + R[i];
  }
  res <- list(U, R, C)
  names(res) <- c("U", "R", "C")
  return(as.data.frame(res))
}
##Get percentile function
GetPercentile <- function( P, G, tmax ){
  # Determine minimum of |G(Tp[i]) - P[i]|
  np <- length(P);
  Tp <- numeric(np)
  for( i in 1:np) {
    cc <- 100;
    for(t in 1:tmax) {
      if ( abs(G[t] - P[i]) < cc ) {
        c <- t;
        cc <- abs(G[t] - P[i]);
      }
    }
    
    if( P[i] > G[c] ){
      Tp[i] <-  c + (P[i] - G[c]) / (G[c+1] - G[c]);
    } else {
      Tp[i] <- c + (P[i] - G[c]) / (G[c] - G[c-1]);
    }
  }
  return( Tp )
}


##RMi
psq <- probSpace(10); psq

dfx <- ties(cx)
dfy <- ties(cy)
dfz <- ties(cz)
tmax <- max(cx,cy,cz)

gx <- cdf.ulrich(data=dfx, maximum=tmax)
gy <- cdf.ulrich(data=dfy, maximum=tmax)
gz <- cdf.ulrich(data=dfz, maximum=tmax)

b <- gx + gy
xp <- GetPercentile(psq, gx, tmax)
yp <- GetPercentile(psq, gy, tmax)
zp <- GetPercentile(psq, gz, tmax);
bp <- GetPercentile(psq, b, tmax);

gdf <- data.frame(RT =c(xp,yp,zp,bp), Probability =rep(psq, 4),
                  Condition =rep(c("gx(t)", "gy(t)","gz(t)","gx(t)+gy(t)"), each=length(xp)))
panelf <- ggplot(gdf, aes(x = RT, y = Probability, group=Condition,
                          colour=Condition, shape=Condition)) + 
  geom_point() + geom_line() 
panelf + coord_cartesian(xlim = c(100, 750), ylim=c(-.01,1.01)) +
  theme(legend.position= c(.85, .20),  
        legend.title = element_text(size=12),
        legend.text = element_text(size=12))
zp <- GetPercentile(psq, gz, tmax);
bp <- GetPercentile(psq, b, tmax);

#############################
### Race Model Inequality ###
#############################

#need to tweak code so that it is applicable to p(F[redundant] > f) > p(F[auditory] > f) + p(F[visual] > f)
