###############################################
### Draft Analysis Plan PSYP01 Experiment 1 ###
###############################################

#########################################
### Systems Factorial Technology in R ###
#########################################

    ### Houpt et al. (2013) ###

###########################################
# Survivor and Mean Interaction Contrasts #
###########################################

# Simulate single channel response times
T1.h <- rweibull(300, shape=2, scale=400)
T1.l <- rweibull(300, shape=2, scale=800)
T2.h <- rweibull(300, shape=2, scale=400)
T2.l <- rweibull(300, shape=2, scale=800)

# Combine into "observed" response times
# assuming a Serial-AND model
hh <- T1.h + T2.h
hl <- T1.h + T2.l
lh <- T1.l + T2.h
ll <- T1.l + T2.l
# Run the SIC analysis
SerialAND <- sic(hh,hl,lh,ll)
SerialAND$SIC

SerialAND$Dominance
SerialAND$positive
SerialAND$negative
SerialAND$MIC

plot(SerialAND$SIC, do.p=FALSE)

########################
# Capacity coefficient #
########################

#OR-coefficient

# Generate single source response times
rate1 <- 1/800
rate2 <- 1/600
pa <- rweibull(100, shape=2,
                 scale=1/rate1)
ap <- rweibull(100, shape=2,
                 scale=1/rate2)

# Simulate a limited capacity
# Parallel-OR model
# Limited capacity means slower
# processing when there are multiple
# sources, so we use .5 times the
# original rate.
pp.1 <- rweibull(100, shape=2,
                   scale=1/(.5*rate1))
pp.2 <- rweibull(100, shape=2,
                   scale=1/(.5*rate2))

pp <- pmin( pp.1, pp.2 )

# Run the capacity analysis
cap <- capacity.or(list(pp, pa, ap))
cap$Ctest

plot(cap$Ct, xlim=c(0,2000), ylim=c(0,2))

#AND-coefficient

#############################
### Race Model Inequality ###
#############################

#Testing the race model inequality in R 
#https://stats.stackexchange.com/questions/48581/testing-the-race-model-inequality-in-r

#example reaction times for single hypothetical participants for 3 experimental  conditions
#this is the same dat set as one used by Urlich et al 2007
cx <- c(244, 249, 257, 260, 264, 268, 271, 274, 277, 291)
cy <- c(245, 246, 248, 250, 251, 252, 253, 254, 255, 259, 263, 265, 279, 282, 284, 319)
cz <- c(234, 238, 240, 240, 243, 243, 245, 251, 254, 256, 259, 270, 280)

#here I calculate quantiles corresponding to the given probabilities
#I use the default quantile algorithm (type 7 see R manual for 'quantile')
#it seems right, but I'm not 100% sure
psq <- seq(0,1,0.1)
gx <- quantile(cx, probs=psq, names=F)
gy <- quantile(cy, probs=psq, names=F)
gz <- quantile(cz, probs=psq, names=F)

#here I visualize the quantiles, looks more or less like in Urlich etal 2007
xyz <- data.frame(rt=c(gx,gy,gz), 
                  perc=rep(psq,3), 
                  cond=rep(c("gx(t)", "gy(t)","gz(t)"), each=length(psq)))
require(lattice)
xyplot(perc ~ rt, groups = cond, xyz, col=1, lty=3:1, type=c("g","l","p"), 
       key=list(lines=list(lty=3:1), text=list(labels=levels(xyz$cond))))


#Compute the bounding um 'b' from g(x) and g(y)
b = gx + gy
#but then when I visualize it, it's wrong:

xyz <- data.frame(rt=c(gx,gy,gz,b), 
                  perc=rep(psq,4), 
                  cond=rep(c("gx(t)", "gy(t)","gz(t)","gx(t)+gy(t)"), each=length(psq)))

xyplot(perc ~ rt, groups = cond, xyz, col=1, lty=4:1, type=c("g","l","p"), 
       key=list(lines=list(lty=4:1), text=list(labels=levels(xyz$cond))))


#############Alternative solution

#Probspace function
probSpace <- function(len){ 
  # PROBSPACE Determine the interval of percentiles 
  #   The function used equation (3) in Ulrich, Miller and Schroter (2007)
  P <- numeric(len);
  for(i in 1:len){
    P[i] <- (i - .5) / len;
  }
  return(P)
}


##CDF function
cdf.ulrich <- function(data=NULL, maximum=3000){
  # Create a container, whose length is the longest data vector
  # data is the output from `ties` function
  U <- data[,1]
  R <- data[,2]
  C <- data[,3]
  G <- numeric(maximum);
  
  
  # The length of the processed data vector, trimming off ties, if there is any.
  k <- length(U);  # U contains data in millisecond, e.g., 320 ms etc. 
  
  # The last element of the cumulative frequency supposely is the 
  # length of the data vector.
  n <- C[k]
  
  for(i in 1:k) { U[i] <- round(U[i]) }
  
  # from 1 ms to the first value of the data set, their probability should be 0.
  for(t in 1:U[1]) { G[t] <- 0 }   
  
  for(t in U[1]:U[2]){
    G[t] <- ( R[1]/2 + (R[1]+R[2]) / 2*(t-U[1]) / (U[2] - U[1]) ) / n;
  }
  
  for(i in 2:(k-1)){
    for(t in U[i]:U[i+1]){
      G[t] <- (C[i-1] + R[i] / 2+(R[i] +R[i+1]) / 2*(t-U[i]) / (U[i+1] - U[i])) / n;
    }
  }
  
  for(t in U[k]:maximum){
    G[t] <- 1;
  }
  return(G)
}

##ties function
ties <- function(W){
  # Count number k of unique values
  # and store these values in U.
  U <- NULL; W <- sort(W); n = length(W); k = 1; U[1] <- W[1]
  for (i in 2:n) {
    if (W[i] != W[i-1]) {
      k <- k+1;
      U <- cbind(U, W[i])
    }
  }
  U <- U[1,]
  
  # Determine number of replications R
  # k is the length of the vector, after trimming off the ties
  R <- numeric(k) 
  for (i in 1:k){
    for (j in 1:n){
      if (U[i] == W[j]) R[i] <- R[i] + 1;
    }
  }
  
  # Determine the cumlative frequency
  C <- numeric(k) 
  C[1] <- R[1]
  for(i in 2:k){
    C[i] <- C[i-1] + R[i];
  }
  res <- list(U, R, C)
  names(res) <- c("U", "R", "C")
  return(as.data.frame(res))
}
##Get percentile function
GetPercentile <- function( P, G, tmax ){
  # Determine minimum of |G(Tp[i]) - P[i]|
  np <- length(P);
  Tp <- numeric(np)
  for( i in 1:np) {
    cc <- 100;
    for(t in 1:tmax) {
      if ( abs(G[t] - P[i]) < cc ) {
        c <- t;
        cc <- abs(G[t] - P[i]);
      }
    }
    
    if( P[i] > G[c] ){
      Tp[i] <-  c + (P[i] - G[c]) / (G[c+1] - G[c]);
    } else {
      Tp[i] <- c + (P[i] - G[c]) / (G[c] - G[c-1]);
    }
  }
  return( Tp )
}


##RMi
psq <- probSpace(10); psq

dfx <- ties(cx)
dfy <- ties(cy)
dfz <- ties(cz)
tmax <- max(cx,cy,cz)

gx <- cdf.ulrich(data=dfx, maximum=tmax)
gy <- cdf.ulrich(data=dfy, maximum=tmax)
gz <- cdf.ulrich(data=dfz, maximum=tmax)

b <- gx + gy
xp <- GetPercentile(psq, gx, tmax)
yp <- GetPercentile(psq, gy, tmax)
zp <- GetPercentile(psq, gz, tmax);
bp <- GetPercentile(psq, b, tmax);

gdf <- data.frame(RT =c(xp,yp,zp,bp), Probability =rep(psq, 4),
                  Condition =rep(c("gx(t)", "gy(t)","gz(t)","gx(t)+gy(t)"), each=length(xp)))
panelf <- ggplot(gdf, aes(x = RT, y = Probability, group=Condition,
                          colour=Condition, shape=Condition)) + 
  geom_point() + geom_line() 
panelf + coord_cartesian(xlim = c(230, 330), ylim=c(-.01,1.01)) +
  theme(legend.position= c(.85, .20),  
        legend.title = element_text(size=12),
        legend.text = element_text(size=12))
zp <- GetPercentile(psq, gz, tmax);
bp <- GetPercentile(psq, b, tmax);
