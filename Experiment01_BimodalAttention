###############################################
### Draft Analysis Plan PSYP:01 Experiment 1 ###
###############################################

# Set up workspace
setwd("C:/R.working.directory/")
rm(list=ls())
bugsdir <- "C:/WinBUGS14" # Download WinBUGS from https://www.mrc-bsu.cam.ac.uk/software/bugs/

#Packages
library(ggplot2)
library(retimes)
library(tidyverse)
library(BayesFactor)
library(sft)
library(polspline)
library(R2WinBUGS)

####################
# Simulate Dataset #
####################



#Set trials/subject and units random draws from exgauss

n            <- 1
trials_per_n <- 180
trials       <- trials_per_n * n


#soft
S <- rexgauss(trials, mu = 430, sigma=90, tau=200, positive = T)
S <- S[S > 0]

#loud
L <- rexgauss(trials, mu = 360, sigma=90, tau=200, positive = T)
L <- L[L > 0]

#dim
D <- rexgauss(trials, mu = 480, sigma=90, tau=200, positive = T)
D <- D[D > 0]

#bright
B <- rexgauss(trials, mu = 420, sigma=90, tau=200, positive = T)
B <- B[B > 0]

#soft + dim
SD <- rexgauss(trials, mu = 400, sigma=90, tau=200, positive = T)
SD <- SD[SD > 0]

#soft + bright
SB <- rexgauss(trials, mu = 360, sigma=90, tau=200, positive = T)
SB <- SB[SB > 0]

#loud + dim
LD <- rexgauss(trials, mu = 350, sigma=90, tau=200, positive = T)
LD <- LD[LD > 0]

#loud + bright
LB <- rexgauss(trials, mu = 320, sigma=90, tau=200, positive = T)
LB <- LB[SD > 0]


#Create dataframe
data = as.data.frame(c(S, L, D, B, SD, LB, LD, SB))
energy = c(rep("soft", n), rep("loud", n), rep("dim", n), rep("bright", n),
           rep("soft", n), rep("loud", n), rep("dim", n), rep("bright", n))
redundancy = c(rep("unimodal", n*4), rep("redundant", n*4))
rt.data = cbind(data, energy, redundancy)
names(rt.data) = c("reaction_time", "stimulus", "redundancy")


##################
# Bayesian ANOVA #
##################

#Compute Bayes Factors
bayes_factor <- anovaBF(reaction_time ~ stimulus*redundancy, data=rt.data)
print(bayes_factor)

bayes_factor@bayesFactor$bf


##################################
# Inverse Effectiveness Contrast #
##################################

# Calculate redundancy gains
weak_gain    <- (((S + D)/2) - SD)
strong_gain  <- (((L + B)/2) - LB)


# Contrast redundancy gains and standardize them
x <- weak_gain - strong_gain 
x <- x/sd(x)      

#Set number of subjects
ndata <- length(x)

# To be passed on to WinBUGS
data  <- list("x", "ndata") 

myinits <- list(
  list(deltatmp = runif(1), sigmatmp = runif(1)),
  list(deltatmp = runif(1), sigmatmp = runif(1)),
  list(deltatmp = runif(1), sigmatmp = runif(1)))

# Parameters to be monitored
parameters <- c("delta")

# Call WinBUGS from R
samples <- bugs(data, inits=myinits, parameters,
                model.file="Gaussian.txt",
                n.chains=3, n.iter=10000, n.burnin=1000, n.thin=1,
                DIC=T, bugs.directory=bugsdir,
                codaPkg=F, debug=T)


samples$summary # overview

# Collect posterior samples across all chains:
delta.posterior <- samples$sims.list$delta  

#============ BFs based on logspline fit ===========================
library(polspline) # this package can be installed from within R
fit.posterior <- logspline(delta.posterior)

#============ BFs based on logspline fit ===========================
fit.posterior <- logspline(delta.posterior,ubound=0) # NB. note the bound

# 95% confidence interval:
x0 <- qlogspline(0.025,fit.posterior)
x1 <- qlogspline(0.975,fit.posterior)

posterior <- dlogspline(0, fit.posterior) # this gives the pdf at point delta = 0
prior     <- 2*dcauchy(0)                 # height of order--restricted prior at delta = 0
BF01      <- posterior/prior
BF01

#============ Plot Prior and Posterior  ===========================
par(cex.main = 1.5, mar = c(5, 6, 4, 5) + 0.1, mgp = c(3.5, 1, 0), cex.lab = 1.5,
    font.lab = 2, cex.axis = 1.3, bty = "n", las=1)
xlow  <- -3
xhigh <- 0
yhigh <- 12
Nbreaks <- 80
y <- hist(delta.posterior, Nbreaks, plot=F)
plot(c(y$breaks, max(y$breaks)), c(0,y$density,0), type="S", lwd=2, lty=2,
     xlim=c(xlow,xhigh), ylim=c(0,yhigh), xlab=" ", ylab="Posterior Density", main = "Redundancy Contrast", axes=F) 
axis(1, at = c(-3,-2,-1,0), lab=c("-3","-2","-1","0"))
axis(2)
mtext(expression(delta), side=1, line = 2.8, cex=2)
#now bring in log spline density estimation:
par(new=T)
plot(fit.posterior, ylim=c(0,yhigh), xlim=c(xlow,xhigh), lty=1, lwd=1, axes=F)
points(0, dlogspline(0, fit.posterior),pch=19, cex=2)
# plot the prior:
par(new=T)
plot ( function( x ) 2*dcauchy( x, 0, 1 ), xlow, xhigh, ylim=c(0,yhigh), xlim=c(xlow,xhigh), lwd=2, lty=1, ylab=" ", xlab = " ", axes=F) 
axis(1, at = c(-3,-2,-1,0), lab=c("-3","-2","-1","0"))
axis(2)
points(0, 2*dcauchy(0), pch=19, cex=2)

###########################################
# Survivor and Mean Interaction Contrasts #
###########################################

#Mean and Survivor Contrasts + Capacity Analysis as devised by 'SFT in R' (Houpt et al.2013)
#http://www.indiana.edu/~psymodel/papers/HouBla13.pdf

# Simulate single channel response times and combine into "observed" response times
# assuming a Serial-AND model
T1.h <- B
T1.l <- D
T2.h <- L
T2.l <- S

hh <- T1.h + T2.h
hl <- T1.h + T2.l
lh <- T1.l + T2.h
ll <- T1.l + T2.l

# Run the SIC analysis
SerialAND <- sic(hh,hl,lh,ll)

SerialAND$SIC

SerialAND$Dominance

SerialAND$positive

SerialAND$negative
SerialAND$MIC

plot(SerialAND$SIC, do.p=FALSE)

## Or enter proper data

hh <- LB
hl <- LD
lh <- SB
ll <- SD

# Run the SIC analysis
SerialAND <- sic(hh,hl,lh,ll)

SerialAND$SIC
SerialAND$Dominance
SerialAND$positive
SerialAND$negative
SerialAND$MIC

plot(SerialAND$SIC, do.p=FALSE)



########################
# Capacity coefficient #
########################

# OR-coefficient #

# Run the capacity analysis for weak signal energies
cap <- capacity.or(list(SD, S, D))
cap$Ctest

plot(cap$Ct, xlim=c(0,2000), ylim=c(0,5))


# Run the capacity analysis for strong signal energies
cap <- capacity.or(list(LB, L, B))
cap$Ctest

plot(cap$Ct, xlim=c(0,2000), ylim=c(0,5))

#AND-coefficient

#############################
### Race Model Inequality ###
#############################

# Race Model Inequality as devised by Ulrich et al. (2007) 
# https://link.springer.com/article/10.3758/BF03193160?LI=true

# RMI algorithm kindly translated from MatLab to R by Y. Lin & L. Piwek via StackExchange. 
# https://stats.stackexchange.com/questions/48581/testing-the-race-model-inequality-in-r

# cx = channel 1, cy = channel 2, cz=redundant trials
cx <- S
cy <- D
cz <- SD

#calculate quantiles corresponding to the given probabilities

psq <- seq(0,1,0.1)
gx <- quantile(cx, probs=psq, names=F)
gy <- quantile(cy, probs=psq, names=F)
gz <- quantile(cz, probs=psq, names=F)

#here I visualize the quantiles, looks more or less like in Urlich etal 2007
xyz <- data.frame(rt=c(gx,gy,gz), 
                  perc=rep(psq,3), 
                  cond=rep(c("gx(t)", "gy(t)","gz(t)"), each=length(psq)))
require(lattice)
xyplot(perc ~ rt, groups = cond, xyz, col=1, lty=3:1, type=c("g","l","p"), 
       key=list(lines=list(lty=3:1), text=list(labels=levels(xyz$cond))))


#Compute the bounding um 'b' from g(x) and g(y)
b = gx + gy
#but then when I visualize it, it's wrong:

xyz <- data.frame(rt=c(gx,gy,gz,b), 
                  perc=rep(psq,4), 
                  cond=rep(c("gx(t)", "gy(t)","gz(t)","gx(t)+gy(t)"), each=length(psq)))

xyplot(perc ~ rt, groups = cond, xyz, col=1, lty=4:1, type=c("g","l","p"), 
       key=list(lines=list(lty=4:1), text=list(labels=levels(xyz$cond))))


#############Alternative solution

#Probspace function
probSpace <- function(len){ 
  # PROBSPACE Determine the interval of percentiles 
  #   The function used equation (3) in Ulrich, Miller and Schroter (2007)
  P <- numeric(len);
  for(i in 1:len){
    P[i] <- (i - .5) / len;
  }
  return(P)
}


##CDF function
cdf.ulrich <- function(data=NULL, maximum=3000){
  # Create a container, whose length is the longest data vector
  # data is the output from `ties` function
  U <- data[,1]
  R <- data[,2]
  C <- data[,3]
  G <- numeric(maximum);
  
  
  # The length of the processed data vector, trimming off ties, if there is any.
  k <- length(U);  # U contains data in millisecond, e.g., 320 ms etc. 
  
  # The last element of the cumulative frequency supposely is the 
  # length of the data vector.
  n <- C[k]
  
  for(i in 1:k) { U[i] <- round(U[i]) }
  
  # from 1 ms to the first value of the data set, their probability should be 0.
  for(t in 1:U[1]) { G[t] <- 0 }   
  
  for(t in U[1]:U[2]){
    G[t] <- ( R[1]/2 + (R[1]+R[2]) / 2*(t-U[1]) / (U[2] - U[1]) ) / n;
  }
  
  for(i in 2:(k-1)){
    for(t in U[i]:U[i+1]){
      G[t] <- (C[i-1] + R[i] / 2+(R[i] +R[i+1]) / 2*(t-U[i]) / (U[i+1] - U[i])) / n;
    }
  }
  
  for(t in U[k]:maximum){
    G[t] <- 1;
  }
  return(G)
}

##ties function
ties <- function(W){
  # Count number k of unique values
  # and store these values in U.
  U <- NULL; W <- sort(W); n = length(W); k = 1; U[1] <- W[1]
  for (i in 2:n) {
    if (W[i] != W[i-1]) {
      k <- k+1;
      U <- cbind(U, W[i])
    }
  }
  U <- U[1,]
  
  # Determine number of replications R
  # k is the length of the vector, after trimming off the ties
  R <- numeric(k) 
  for (i in 1:k){
    for (j in 1:n){
      if (U[i] == W[j]) R[i] <- R[i] + 1;
    }
  }
  
  # Determine the cumlative frequency
  C <- numeric(k)  
  C[1] <- R[1]
  for(i in 2:k){
    C[i] <- C[i-1] + R[i];
  }
  res <- list(U, R, C)
  names(res) <- c("U", "R", "C")
  return(as.data.frame(res))
}
##Get percentile function
GetPercentile <- function( P, G, tmax ){
  # Determine minimum of |G(Tp[i]) - P[i]|
  np <- length(P);
  Tp <- numeric(np)
  for( i in 1:np) {
    cc <- 100;
    for(t in 1:tmax) {
      if ( abs(G[t] - P[i]) < cc ) {
        c <- t;
        cc <- abs(G[t] - P[i]);
      }
    }
    
    if( P[i] > G[c] ){
      Tp[i] <-  c + (P[i] - G[c]) / (G[c+1] - G[c]);
    } else {
      Tp[i] <- c + (P[i] - G[c]) / (G[c] - G[c-1]);
    }
  }
  return( Tp )
}


##RMi
psq <- probSpace(10); psq

dfx <- ties(cx)
dfy <- ties(cy)
dfz <- ties(cz)
tmax <- max(cx,cy,cz)

gx <- cdf.ulrich(data=dfx, maximum=tmax)
gy <- cdf.ulrich(data=dfy, maximum=tmax)
gz <- cdf.ulrich(data=dfz, maximum=tmax)

b <- gx + gy
xp <- GetPercentile(psq, gx, tmax)
yp <- GetPercentile(psq, gy, tmax)
zp <- GetPercentile(psq, gz, tmax);
bp <- GetPercentile(psq, b, tmax);

gdf <- data.frame(RT =c(xp,yp,zp,bp), Probability =rep(psq, 4),
                  Condition =rep(c("gx(t)", "gy(t)","gz(t)","gx(t)+gy(t)"), each=length(xp)))
panelf <- ggplot(gdf, aes(x = RT, y = Probability, group=Condition,
                          colour=Condition, shape=Condition)) + 
  geom_point() + geom_line() 
panelf + coord_cartesian(xlim = c(500, 750), ylim=c(-.01,1.01)) +
  theme(legend.position= c(.85, .20),  
        legend.title = element_text(size=12),
        legend.text = element_text(size=12))
zp <- GetPercentile(psq, gz, tmax);
bp <- GetPercentile(psq, b, tmax);
